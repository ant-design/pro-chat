---
title: Why use ProChat
group:
  title: Get Started
nav:
  title: Documents
  order: 0
---

# Why use ProChat

The students who can reach here are all interested in seeing what ProChat is and what situations it may need to be used in business.

Let's put aside the pre-existing reasons for the popularity of AI or LLM and approach this issue from the perspective of a front-end developer.

## The complex structure of large models

The structure of a large model cannot be said to be very complex, it can only be said to be very complex. There are many proprietary terms of artificial intelligence inside, and the cost of a front-end developer understanding and learning the knowledge inside is too high.

But from the perspective of "front-end pages," front-end developers are actually most concerned about "dialogue."

Detailed entry and exit parameters can be found in [Introduction to Large Model Parameters](../request-params-intro) [Introduction to Large Model Parameters](../return-parameters-intro)

## ProChat Experience Details

### Default streaming output support

The streaming output implemented by ChatGPT is far superior to traditional HTTP requests in terms of user experience, and SSE (Server Send Event) technology can also be said to have officially entered the mainstream technology circle's field of vision.

As a front-end solution for AI sessions, ProChat naturally integrates the ability of streaming output by default. Simply configure a Response (Web standard Response object) in the request that returns streaming text to easily integrate streaming effects.

The effect is as follows

<img src="https://mdn.alipayobjects.com/huamei_re70wt/afts/img/A*0uQhSIzSS3YAAAAAAAAAAAAADmuEAQ/original" style="width:100%;">

Similarly, ProChat's request API is also compatible with traditional non streaming requests:

```js
import { ProChat } from '@ant-design/pro-chat';
import { useTheme } from 'antd-style';

const delay = (text: string) =>
  new Promise<string>((resolve) => {
    setTimeout(() => {
      resolve(text);
    }, 5000);
  });

export default () => {
  const theme = useTheme();
  return (
    <div style={{ background: theme.colorBgLayout }}>
      <ProChat
        request={async (messages) => {
          const text = await delay(
            `这是一条模拟非流式输出的消息的消息。本次会话传入了${messages.length}条消息`,
          );
          return new Response(text);
        }}
        style={{ height: '300px' }}
        />
    </div>
  );
};

```

### Multiple rendering support

Another headache for the front-end is [parsing rendering]

Like the following, if you write it yourself, you need to parse the content of the String section and decide which to use and what to render. We have already built-in some renderers for you: Markdown rendering, terminal command concatenation, jump links, and other commonly used renderers

<code src="./demos/doc-mode.tsx" ></code>

For multi line code blocks, we have strengthened the interaction ability of code block components, making them have advanced functions such as folding and unfolding, and changing highlighted languages, thereby helping you better view the code generated by AI in daily use of AI models.

<img src=" https://mdn.alipayobjects.com/huamei_re70wt/afts/img/A*e4JbQKfupVQAAAAAAAAAAAAADmuEAQ/original" style="width:100%;">

### Quick editing, retry, and more capabilities

If you ask the wrong question? I want to start somewhere and modify my previous question? Or do I think his response is not very good, and I would like to help him modify his answer for future questions?

We support the ability to quickly edit, delete, and regenerate, all of which are integrated into the `ProChat` component. Developers do not need to have the mental burden of how to implement these capabilities, because we have maintained this data flow for you!
